\documentclass[../main.tex]{subfiles}

\section{Optimization solutions}

\subsection{Producer optimization} \label{a:producer_optimization}

Let,

\begin{equation}
    J_{\psi}(s, p, r) = s \cdot (p - k) - c(s, r) \cdot r + \beta \cdot V(s + r, \psi \cdot p)
\end{equation}

such that, for a given $\psi$, Equation (\ref{bellman_prod}) can be written as,

\begin{equation}
    V(s, p) = \max_{r_t \in [\underline{r}, \bar{r}]} J_{\psi}(s, p, r).
\end{equation}

Maximization of $J_\psi$ requires (first order condition),

\begin{equation} \label{J_foc_r}
    \frac{\partial J_{\psi}}{\partial r}(s, p, r)  = -\frac{\partial c}{\partial r}(s, r) \cdot r - c(s, r) + \beta \cdot \frac{\partial V}{\partial s}(s + r, \psi \cdot p) = 0
\end{equation}

Assuming maximization in $r$, by the envelope theorem,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= (p - k) - \frac{\partial c}{\partial s}(s, r) \cdot r + \beta \cdot \frac{\partial V}{\partial s}(s + r, \psi \cdot p)
    \end{split}
\end{equation}

Using Equation (\ref{J_foc_r}) on the first condition yields,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= (p - k) - \frac{\partial c}{\partial s}(s, r) \cdot r + \frac{\partial c}{\partial r}(s, r) \cdot r + c(s, r) \\
        &= (p - k) + \left[\frac{\partial c}{\partial r}(s, r) - \frac{\partial c}{\partial s}(s, r)\right] \cdot r + c(s, r)
    \end{split}
\end{equation}

Iterating forward yields,

\begin{equation}
    \frac{\partial V}{\partial s}(s + r, \psi \cdot p)  = \psi \cdot p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \cdot r + c(s + r, r)
\end{equation}

Using (\ref{J_foc_r}),

\begin{equation}
    \frac{\frac{\partial c}{\partial r}(s, r) \cdot r + c(s, r)}{\beta} =  \psi \cdot p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \cdot r + c(s + r, r)
\end{equation}

\subsection{Stability of different cost functions}

The first order condition for $r$ requires that,

\begin{equation}
    \frac{\frac{\partial c}{\partial r}(s, r) \cdot r + c(s, r)}{\beta} =  \psi \cdot p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \cdot r + c(s + r, r)
\end{equation}

Any cost function with the trivial property that not ramping up is free, $c(s, 0) = 0$, is stable, $r = 0$, if and only if,


\begin{equation}
    \psi \cdot p = k
\end{equation}

\subsection{Limiting behaviour as $c_1 \xrightarrow{} \infty$}\label{a:limiting}
\newcommand{\limc}{\lim_{c_1 \xrightarrow{} \infty}}

Assume a softplus cost function and notice that, letting

\begin{equation*}
    \sigm_{c_1}(x) = \frac{1}{1 + \exp(-c_1 \cdot x)},
\end{equation*}

we can write,

\begin{equation}
    \frac{\partial c}{\partial s}(s, r) = \sigm_{c_1}(s \cdot r) \cdot r \ \text{ and } \ \frac{\partial c}{\partial r}(s, r) = \sigm_{c_1}(s \cdot r) \cdot s
\end{equation}


Hence the two function that regulate the first order condition of the prosumer can be rewritten as,

\begin{equation}
    \begin{split}
        mc(r) &= \frac{\partial c}{\partial r}(s_t, r) \cdot r + c(s_t, r) \\
        &= \sigm_{c_1}(s_t \cdot r) \cdot s_t \cdot r_t + c(s_t, r)
    \end{split}
\end{equation}

and

\begin{equation}
    \begin{split}
        mb(r) &= \psi \cdot p_t - k + \left[\frac{\partial c}{\partial r}(s_t + r, r) - \frac{\partial c}{\partial s_t}(s_t + r, r)\right] \cdot r + c(s_t + r, r)\\
        &= \psi \cdot p_t - k + \left[\sigm_{c_1}(.) \cdot (s_t + r) - \sigm_{c_1}(.) \cdot r \right] \cdot r + c(s_t + r, r) \\
        &=\psi \cdot p_t - k + \sigm_{c_1}((s_t + r) \cdot r) \cdot s_t \cdot r + c(s_t + r, r)
    \end{split}
\end{equation}

We can then study the behaviour of the two functions as $c_1 \xrightarrow{} \infty$. Given our model we can assume $s_t \geq 0$. First, notice that,

\begin{equation}
    \limc \sigm_{c_1}(x) = \begin{cases}
        1 & \text{if } x \geq 0 \\
        0 & \text{if } x < 0
    \end{cases}
\end{equation}

We can use this and the fact that $\limc c(s, r) = \max\{0, s \cdot r\}$, to evaluate,

\begin{equation}
    \limc mc(r) = \max \{0, 2s_t \cdot r\}
\end{equation}

and,

\begin{equation*}
    \limc mb(r) = \psi \cdot p_t - k + \max\{0, (s_t + r) \cdot r\} + \begin{cases}
        s_t \cdot r &\text{if } r \in (-\infty, -s_t] \cup [0, \infty) \\
        0 &\text{if } r \in (-s_t, 0)
    \end{cases}
\end{equation*}

Using the optimization constraint, $r \in [-s_t, \infty)$, we obtain,

\begin{equation}
    \limc mb(r) = \psi \cdot p_t - k + \begin{cases}
        2 s_t \cdot r + r^2 &\text{if } r > 0 \\
        0 &\text{if } r \in [-s_t, 0]
    \end{cases}
\end{equation}

Then, in the limit, we can rewrite the first order condition as,

\begin{equation*}
    \begin{split}
        \limc mc(r) &= \beta \limc mb(r) \\
        \frac{1}{\beta} \begin{cases}
            2 s_t \cdot r  &\text{if } r > 0 \\
            0 &\text{if } r \in [-s_t, 0]
        \end{cases} &= \psi \cdot p_t - k  + \begin{cases}
            2 s_t \cdot r + r^2 &\text{if } r > 0 \\
            0 &\text{if } r \in [-s_t, 0]
        \end{cases}
    \end{split}
\end{equation*}

This allows us to find an approximate policy function. Namely, assuming producing is profitable, $\psi \cdot p_t - k > 0$ and $s_t  > \frac{\beta}{1-\beta} \sqrt{\psi \cdot p_t - k}$, we have the interior solution,

\begin{equation}
    r(s_t, p_t; \psi) = \frac{1}{2} \cdot \left[  \left(\frac{1 - \beta}{\beta} \cdot 2 s_t \right) - \sqrt{\left(\frac{1 - \beta}{\beta} \cdot 2 s_t \right)^2 - 4 \cdot \left(\psi \cdot p_t - k\right)} \right].
\end{equation}

If, on the other hand, $s_t \leq \frac{\beta}{1-\beta} \sqrt{\psi \cdot p_t - k}$, then $\beta \cdot mb(r) > mc(r)$, hence,

\begin{equation}
    r(s_t, p_t; \psi) = \frac{\beta}{1-\beta} \sqrt{\psi \cdot p_t - k} - s_t
\end{equation}

Finally, if $\psi \cdot p_t - k < 0$

\begin{equation}
    r(s_t, p_t; \psi) = -\gamma \cdot s
\end{equation}

\subsection{Evolution of excess demand} \label{a:ev_demand}

Using the definition of $X_t$, equation (\ref{x_inst}), we can rewrite,

\begin{equation}
    \begin{split}
        X_{t+1} &= M \cdot e_{t+1} - S_{t+1} \\
        S_{t+1} &= M \cdot e_{t+1} - X_{t+1} = R_t + S_t
    \end{split}
\end{equation}

hence,

\begin{equation}
    \begin{split}
        X_{t+1} - M \cdot e_{t+1} &= X_t - M \cdot e_t - R_t \\
        X_{t+1} &= X_t + M \cdot \left(e_{t+1} - e_t \right) -  R_t
    \end{split}
\end{equation}

\subsection{Provider optimization} \label{a:provider_optimization}

\subsubsection{Euler equation}

First assume that $X_t > 0$. The case for $X_t < 0$ is perfectly symmetric. Let the approximated ramp-up function be,

\begin{equation}
    \tilde{R}(p) \coloneqq a + b \cdot p \cong R(p)
\end{equation}

Each period the provider faces the problem,

\begin{equation*}
    \begin{split}
        \mathcal{L}(p, \Y, X) &= p \cdot X - \sum_{j \in N_{\mathcal{A}}(i)} Y^j_t \cdot P^j(\Y) \\
        &+ \lambda_t \cdot \left( X -  \sum_{j} Y_t^j \right) \\
        &+ \beta \cdot  V\left(X - \tilde{R} (p) \right)
    \end{split}
\end{equation*}

such that $V(X) = \max_{p, \Y} \mathcal{L}(p, \Y, X)$. The first order condition of $\mathcal{L}$ requires that,

\begin{equation} \label{foc_p}
    \begin{split}
        \frac{\partial}{\partial p} \mathcal{L} &= X - \beta \cdot V^\prime \left(X - \tilde{R}(p) \right) \cdot \tilde{R}^\prime(p) = 0 \\
        X &= \beta \cdot V^\prime \left(X - \tilde{R}(p) \right) \cdot \frac{\partial \tilde{R}}{\partial p}(p)
    \end{split}
\end{equation}

This first order condition implies that the benefit of charging a higher price today, which is today's demand $X$, should be equal to the cost of reducing the demand tomorrow, $\beta \cdot V^\prime \left(X - \tilde{R}(p) \right) \cdot \frac{\partial \tilde{R}}{\partial p}(p)$. The second first order condition requires that, for each neighbor $m$, 

\begin{equation} \label{foc_Y}
    \begin{split}
        \frac{\partial}{\partial Y^m} \mathcal{L} &= \sum_{j \in N_{\mathcal{A}}(i)} \frac{\partial (Y^j \cdot P^j(\Y))}{\partial Y^m} + \lambda = 0
    \end{split}
\end{equation}

We can distribute the partial derivative by noting that for each entry in the sum, $j$ not equal to $m$, we take the partial derivative of $P^j$ with respect to $Y^m$, and for the entry $m$ in the sum we need to use the chain rule. 

\begin{equation} \label{lambda}
    \begin{split}
        -\lambda &= \sum_{j \in N_{\mathcal{A}}(i)} \frac{\partial (Y^j \cdot P^j(\Y))}{\partial Y^m} \\
        &= Y^1 \cdot \frac{\partial P^1(\Y)}{\partial Y^m} + \ldots + \underbrace{\frac{\partial Y^m}{\partial Y^m} P^m(\Y) + Y^m \cdot \frac{\partial P^m(\Y)}{\partial Y^m} }_{\text{by the chain rule}} + Y^{m+1} \cdot \frac{\partial P^{m+1}(\Y)}{\partial Y^m} \ldots \\
        &= P^m(\Y) + \sum_{j \in N_{\mathcal{A}}(i)} Y^j \cdot \frac{\partial P^j(\Y)}{\partial Y^m}
    \end{split}
\end{equation} 

\subsubsection{Envelope}

By the envelope theorem,

\begin{equation} \label{env}
    V^\prime(X_t) = p_t + \lambda_t + \beta \cdot V^\prime(X_{t+1})
\end{equation}


Then combining Equations (\ref{env}) and (\ref{foc_p}), we obtain

\begin{equation}
    V^\prime(X_t) = p_t + \lambda_t + \frac{X_t}{\frac{\partial \tilde{R}}{\partial p}(p_t)}
\end{equation}

Iterating forward,

\begin{equation}
    V^\prime(X_{t+1}) = p_{t+1} + \lambda_{t+1} + \frac{X_{t+1}}{\frac{\partial \tilde{R}}{\partial p}(p_{t+1})}
\end{equation}

Using (\ref{foc_p}),

\begin{equation}
    \frac{X_t}{\beta \cdot \frac{\partial \tilde{R}}{\partial p}(p_t)} = p_{t+1} + \lambda_{t+1} + \frac{X_{t} - \tilde{R}(p_t)}{\frac{\partial \tilde{R}}{\partial p}(p_{t+1})}
\end{equation}

Finally, using the linearity of $\tilde{R}$, $\frac{\partial \tilde{R}}{\partial p} = b$,

\begin{equation}
    \begin{split}
        \frac{X_t}{\beta \cdot b} &=  \lambda_{t+1} + p_{t+1} + \frac{X_t}{b} - \frac{a +  b \cdot p_{t}}{b} \\
        p_{t+1} &= p_t + \frac{a}{b} +  \left( \frac{1 - \beta}{\beta} \right) \cdot \frac{X_t}{b} - \lambda_{t+1}
    \end{split}
\end{equation}

\subsubsection{Bargaining solution} \label{a:barsol}

The Nash bargaining solution is such that

\begin{equation*}
    P^{(i, j)} = \arg \max_{P^{(i, j)}} \left\{\Pi_i \cdot \Pi_j \right\}.
\end{equation*}

The first order condition is such that,

\begin{equation*}
    \frac{\partial}{\partial P^{(i, j)}} \left( \Pi_i \cdot \Pi_j \right) = \frac{\partial \Pi_i}{\partial  P^{(i, j)}} \cdot \Pi_j + \Pi_i \cdot \frac{\partial \Pi_j}{\partial  P^{(i, j)}} = 0
\end{equation*}

Then, by symmetry, $\frac{\partial \Pi_j}{\partial  P^{(i, j)}} = - \frac{\partial \Pi_i}{\partial  P^{(i, j)}}$,

\begin{equation}
    \Pi_j = \Pi_i 
\end{equation}

Note that an equal division of the pie is consistent with the network bargaining model of \citein{Corominas-Bosch2004} with infinitely patient traders.

\subsection{Illustrative examples}
\subsubsection{Complete graph}
Consider the simplest non-trivial example, namely the complete graph with three nodes (Figure \ref{tikz:complete}). The optimization of prosumer one can be written as,

\begin{equation*}
    V(X_{1, t}) = \max_{p_{1, t}, Y_t^{(1, 2)}, Y_t^{(1, 3)}} \left\{ \mathcal{L}\left( p_{1, t}, Y_t^{(1, 2)}, Y_t^{(1, 3)}, X_{1, t} \right) \right\}
\end{equation*}

\begin{figure}[!ht]
    \centering
    \input{sections/diagrams/complete.tikz}
    \caption{Complete graph}
    \label{tikz:complete}
\end{figure}

The Lagrangian multiplier can then be written as,

\begin{equation*}
    -\lambda_1 = P^{(1, 2)} + Y^{(1, 2)} \cdot \frac{\partial P^{(1, 2)}}{\partial Y^{(1, 2)}} + Y^{(1, 3)} \cdot \frac{\partial P^{(1, 3)}}{\partial Y^{(1, 2)}}
\end{equation*}

The optimization of prosumers two and three is equivalent. Using $Y^{(2, 1)} = - Y^{(1, 2)}$, the multipler of two becomes,

\begin{equation*}
    -\lambda_2 = P^{(2, 3)} + Y^{(2, 3)} \cdot \frac{\partial P^{(2, 3)}}{\partial Y^{(2, 3)}} - Y^{(1, 2)} \cdot \frac{\partial P^{(1, 2)}}{\partial Y^{(2, 3)}}
\end{equation*}

Finally, the multiplier of three, noting that $\frac{\partial Y^{(1, 3)}}{\partial Y^{(3, 1)}} = -1$,

\begin{equation*}
    -\lambda_3 = -P^{(1, 3)} - Y^{(1, 3)} \cdot \frac{\partial P^{(1, 3)}}{\partial Y^{(1, 3)}} - Y^{(2, 3)} \cdot \frac{\partial P^{(2, 3)}}{\partial Y^{(1, 3)}}
\end{equation*}

We can then use the Nash bargaining price (\ref{bargaining_solution}) to define $P$,

\begin{equation*}
    P^{(1, 2)} = \underbrace{\Delta^{(1, 2)}}_{\text{revenue difference}} - \underbrace{Y^{(1, 3)} \cdot P^{(1, 3)}}_{\text{outside option of } 1} + \underbrace{Y^{(2, 3)} \cdot P^{(2, 3)}}_{\text{outside option of } 2}  \Bigg/ 2 Y^{(1, 2)}
\end{equation*}

To get an intuition for the formula, note that if we were to cut the ties between one and three (setting $Y^{(1, 3)}$ to 0), thereby removing one's outside option, we would obtain a higher price $P^{(1, 2)}$.

By stacking the price over every edge in a vector we obtain,

\begin{equation*}
    2 \cdot \underbrace{\begin{pmatrix}
        Y^{(1, 2)} P^{(1, 2)} \\
        Y^{(1, 3)} P^{(1, 3)} \\
        Y^{(2, 3)} P^{(2, 3)}
    \end{pmatrix}}_{\Y \circ P} = 
    \underbrace{\begin{pmatrix}
        \Delta^{(1, 2)} \\
        \Delta^{(1, 3)} \\
        \Delta^{(2, 3)}
    \end{pmatrix}}_{\Delta} - \underbrace{\begin{pmatrix}
        0 & 1 & -1 \\
        -1 & 0 & 1 \\
        1 & -1 & 0
    \end{pmatrix}}_{\matr{G}} \underbrace{\begin{pmatrix}
        Y^{(1, 2)} P^{(1, 2)} \\
        Y^{(1, 3)} P^{(1, 3)} \\
        Y^{(2, 3)} P^{(2, 3)}
    \end{pmatrix}}_{\Y \circ P} 
\end{equation*}

This can be rewritten as,

\begin{equation*}
    \begin{split}
        \left( \begin{pmatrix}
            2 & 0 & 0 \\
            0 & 2 & 0 \\
            0 & 0 & 2
        \end{pmatrix} + \begin{pmatrix}
            0 & 1 & -1 \\
            -1 & 0 & 1 \\
            1 & -1 & 0
        \end{pmatrix} \right) (\Y \circ P) &= \Delta \\
        \begin{pmatrix}
            2 & 1 & -1 \\
            -1 & 2 & 1 \\
            1 & -1 & 2
        \end{pmatrix} (\Y \circ P) &= \Delta \\
        \begin{pmatrix}
            Y^{(1, 2)} & 0 & 0 \\
            0 & Y^{(1, 3)} & 0 \\
            0 & 0 & Y^{(2, 3)}
        \end{pmatrix}
        P
         &= \begin{pmatrix}
            2 & 1 & -1 \\
            -1 & 2 & 1 \\
            1 & -1 & 2
        \end{pmatrix}^{-1} \Delta \\
        P
         = \underbrace{\begin{pmatrix}
            1 / Y^{(1, 2)} & 0 & 0 \\
            0 & 1 / Y^{(1, 3)} & 0 \\
            0 & 0 & 1 / Y^{(2, 3)}
        \end{pmatrix}}_{\diag(\Y)^{-1}} &\begin{pmatrix}
            0.36 & -0.07 & 0.21\\
            0.21 & 0.36 & -0.07\\
            -0.07 & 0.21 & 0.36\\
        \end{pmatrix} \Delta \\
    \end{split}
\end{equation*}

Letting $D_{\Y} \coloneqq \frac{\der}{\der \Y}$ be the derivative with respect to the vector $\Y$. Then taking the derivative in the above expression yields,

\begin{equation}
    \begin{split}
        \der P &= \der \diag(\Y)^{-1} \begin{pmatrix}
            0.36 & -0.07 & 0.21\\
            0.21 & 0.36 & -0.07\\
            -0.07 & 0.21 & 0.36\\
        \end{pmatrix} \Delta \\
        \der P &= -\diag(\Y)^{-1} \der \diag(\Y) \underbrace{\diag(\Y)^{-1} \begin{pmatrix}
            0.36 & -0.07 & 0.21\\
            0.21 & 0.36 & -0.07\\
            -0.07 & 0.21 & 0.36\\
        \end{pmatrix} \Delta}_{P} \\
        \der P &= -\diag(\Y)^{-1} \der \diag(\Y) P \\
        \der P &= -\diag(\Y)^{-1} \diag(P) \der \Y
    \end{split}
\end{equation}

Hence we can write the jacobian of the price vector as,

\begin{equation}
    D_{\Y} P = -\begin{pmatrix}
        P^{(1, 2)} / Y^{(1, 2)} & 0 & 0 \\
        0 & P^{(1, 3)} / Y^{(1, 3)} & 0 \\
        0 & 0 & P^{(2, 3)} / Y^{(2, 3)}
    \end{pmatrix}
\end{equation}

Using the multiplier's definition this allows as to rewrite,

\begin{equation}
    \begin{split}
        -\lambda_1 &= 2P^{(1, 2)} \\
        -\lambda_2 &= 2P^{(2, 3)} \\
        -\lambda_3 &= -2P^{(1, 3)}
    \end{split}
\end{equation}

This yields the evolution of local prices $\matr{p}_{t+1} = \begin{pmatrix}
    p_{1, t+1} \\ p_{2, t+1} \\ p_{3, t+1}   
\end{pmatrix}_t$

\begin{equation}
    \matr{p}_{t+1} = \matr{p}_t + \left( \matr{a} + \frac{1 -\beta}{\beta} \matr{X}_t \right) \oslash \matr{b} - \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & -1
    \end{pmatrix} P_t 
\end{equation}

\subsection{Jacobian of bargaining price} \label{a:jacobian_p}

I will use the fact that,

\begin{equation}
    \diag(\Y) P = \diag(P) \Y = \Y \circ P \implies d \ \diag(\Y) P = \diag(P) \ d \Y
\end{equation}

First let $\matr{D} \coloneqq \diag(\Y)$ and $\matr{A} \coloneqq (2\matr{I} + \matr{G})^{-1} \Delta$. Noting that $\frac{d \matr{A}}{d \Y} = 0$,

\begin{equation}
    \begin{split}
        P &= \matr{D}^{-1} \matr{A} \\
        dP &= d\matr{D}^{-1} \matr{A} \\
        &= -\matr{D}^{-1} d \matr{D} \underbrace{\matr{D}^{-1} \matr{A}}_{P} \\
        &= -\matr{D}^{-1} \diag(P) \ d \Y
    \end{split}
\end{equation}