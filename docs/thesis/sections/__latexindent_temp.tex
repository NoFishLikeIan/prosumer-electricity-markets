\documentclass[../thesis.tex]{subfiles}

\section{Optimization solutions}

\subsection{Producer optimization} \label{a:producer_optimization}

Let

\begin{equation}
    J(s, p, r) = s \  (p - k) - c(s, r) \  r + \beta \  V(s + r, p),
\end{equation}

such that, Equation (\ref{bellman_prod}) can be written as

\begin{equation}
    V(s, p) = \max_{r_t \in [\underline{r}, \bar{r}]} J(s, p, r).
\end{equation}

Maximization of $J$ requires (first order condition)

\begin{equation} \label{J_foc_r}
    \frac{\partial J}{\partial r}(s, p, r)  = -\frac{\partial c}{\partial r}(s, r) \  r - c(s, r) + \beta \  \frac{\partial V}{\partial s}(s + r, p) = 0.
\end{equation}

Assuming maximization in $r$, by the envelope theorem,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= (p - k) - \frac{\partial c}{\partial s}(s, r) \  r + \beta \  \frac{\partial V}{\partial s}(s + r, p).
    \end{split}
\end{equation}

Using Equation (\ref{J_foc_r}) on the first condition yields

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= (p - k) - \frac{\partial c}{\partial s}(s, r) \  r + \frac{\partial c}{\partial r}(s, r) \  r + c(s, r) \\
        &= (p - k) + \left[\frac{\partial c}{\partial r}(s, r) - \frac{\partial c}{\partial s}(s, r)\right] \  r + c(s, r).
    \end{split}
\end{equation}

Iterating forward yields

\begin{equation}
    \frac{\partial V}{\partial s}(s + r, p)  = p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \  r + c(s + r, r).
\end{equation}

Using (\ref{J_foc_r})

\begin{equation}
    \frac{\frac{\partial c}{\partial r}(s, r) \  r + c(s, r)}{\beta} =  p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \  r + c(s + r, r).
\end{equation}

\subsection{Stability of different cost functions}

The first order condition for $r$ requires that

\begin{equation}
    \frac{\frac{\partial c}{\partial r}(s, r) \  r + c(s, r)}{\beta} =  p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \  r + c(s + r, r).
\end{equation}

Any cost function with the trivial property that not ramping up is free, $c(s, 0) = 0$, is stable, $r = 0$, if and only if


\begin{equation}
    p = k
\end{equation}

\subsection{Limiting behavior as $c_1 \xrightarrow{} \infty$}\label{a:limiting}
\newcommand{\limc}{\lim_{c_1 \xrightarrow{} \infty}}

Assume a softplus cost function and notice that, letting

\begin{equation*}
    \sigm_{c_1}(x) = \frac{1}{1 + \exp(-c_1 \  x)},
\end{equation*}

we can write,

\begin{equation}
    \frac{\partial c}{\partial s}(s, r) = \sigm_{c_1}(s \  r) \  r \ \text{ and } \ \frac{\partial c}{\partial r}(s, r) = \sigm_{c_1}(s \  r) \  s.
\end{equation}


Hence the two function that regulate the first order condition of the prosumer can be rewritten as

\begin{equation}
    \begin{split}
        mc(r) &= \frac{\partial c}{\partial r}(s_t, r) \  r + c(s_t, r) \\
        &= \sigm_{c_1}(s_t \  r) \  s_t \  r_t + c(s_t, r)
    \end{split}
\end{equation}

and

\begin{equation}
    \begin{split}
        mb(r) &= p_t - k + \left[\frac{\partial c}{\partial r}(s_t + r, r) - \frac{\partial c}{\partial s_t}(s_t + r, r)\right] \  r + c(s_t + r, r)\\
        &= p_t - k + \left[\sigm_{c_1}(.) \  (s_t + r) - \sigm_{c_1}(.) \  r \right] \  r + c(s_t + r, r) \\
        &=p_t - k + \sigm_{c_1}((s_t + r) \  r) \  s_t \  r + c(s_t + r, r).
    \end{split}
\end{equation}

We can then study the behavior of the two functions as $c_1 \xrightarrow{} \infty$. Given our model we can assume $s_t \geq 0$. First, notice that

\begin{equation}
    \limc \sigm_{c_1}(x) = \begin{cases}
        1 & \text{if } x \geq 0 \\
        0 & \text{if } x < 0
    \end{cases}.
\end{equation}

We can use this and the fact that $\limc c(s, r) = \max\{0, s \  r\}$, to evaluate,

\begin{equation}
    \limc mc(r) = \max \{0, 2s_t \  r\}
\end{equation}

and

\begin{equation*}
    \limc mb(r) = p_t - k + \max\{0, (s_t + r) \  r\} + \begin{cases}
        s_t \  r &\text{if } r \in (-\infty, -s_t] \cup [0, \infty) \\
        0 &\text{if } r \in (-s_t, 0).
    \end{cases}
\end{equation*}

Using the optimization constraint, $r \in [-s_t, \infty)$, we obtain

\begin{equation}
    \limc mb(r) = p_t - k + \begin{cases}
        2 s_t \  r + r^2 &\text{if } r > 0 \\
        0 &\text{if } r \in [-s_t, 0]
    \end{cases}.
\end{equation}

Then, in the limit, we can rewrite the first order condition as

\begin{equation*}
    \begin{split}
        \limc mc(r) &= \beta \limc mb(r) \\
        \frac{1}{\beta} \begin{cases}
            2 s_t \  r  &\text{if } r > 0 \\
            0 &\text{if } r \in [-s_t, 0]
        \end{cases} &= p_t - k  + \begin{cases}
            2 s_t \  r + r^2 &\text{if } r > 0 \\
            0 &\text{if } r \in [-s_t, 0]
        \end{cases}
    \end{split}.
\end{equation*}

This allows us to find an approximate policy function. Namely, assuming producing is profitable, $p_t - k > 0$ and $s_t  > \frac{\beta}{1-\beta} \sqrt{p_t - k}$, we have the interior solution

\begin{equation}
    r(s_t, p_t) = \frac{1}{2} \  \left[  \left(\frac{1 - \beta}{\beta} \  2 s_t \right) - \sqrt{\left(\frac{1 - \beta}{\beta} \  2 s_t \right)^2 - 4 \  \left(p_t - k\right)} \right].
\end{equation}

If, on the other hand, $s_t \leq \frac{\beta}{1-\beta} \sqrt{p_t - k}$, then $\beta \  mb(r) > mc(r)$, hence

\begin{equation}
    r(s_t, p_t) = \frac{\beta}{1-\beta} \sqrt{p_t - k} - s_t.
\end{equation}

Finally, if $p_t - k < 0$

\begin{equation}
    r(s_t, p_t) = -\gamma \  s.
\end{equation}

\subsection{Evolution of excess demand} \label{a:ev_demand}

Using the definition of $X_t$, equation (\ref{x_inst}), we can rewrite

\begin{equation}
    \begin{split}
        X_{t+1} &= M \  e_{t+1} - S_{t+1} \\
        S_{t+1} &= M \  e_{t+1} - X_{t+1} = R_t + S_t
    \end{split}
\end{equation}

hence

\begin{equation}
    \begin{split}
        X_{t+1} - M \  e_{t+1} &= X_t - M \  e_t - R_t \\
        X_{t+1} &= X_t + M \  \left(e_{t+1} - e_t \right) -  R_t
    \end{split}
\end{equation}

\subsection{Provider optimization} \label{a:provider_optimization}

\subsubsection{Euler equation}

First assume that $X_t > 0$. The case for $X_t < 0$ is perfectly symmetric. Let the approximated ramp-up function be

\begin{equation}
    \B[R(p_t, S_t)] = \alpha + \gamma \  p_t + \eta \ S_t.
\end{equation}

Then let

\begin{equation}
    \begin{split}
        X_{t+1} &= X_t - \B[R(p_t, S_t)] \\
        S_{t+1} &= S_t + \B[R(p_t, S_t)] \\
        V_{t+1} &= V(X_{t+1}, S_{t+1})
    \end{split}
\end{equation}

Each period the provider faces the problem,

\begin{equation*}
    \begin{split}
        \mathcal{L}(p, \Y, X_t, S_t) &= p \  X_t - \sum_{j \in N_{\mathcal{A}}(i)} Y^j \  P^j(\Y) \\
        &+ \lambda_t \  \left( X_t -  \sum_{j} Y^j \right) + \beta \ V_{t+1}
    \end{split}
\end{equation*}

such that $V(X_t, S_t) = \max_{p, \Y} \mathcal{L}(p, \Y, X_t, S_t)$. The first order condition of $\mathcal{L}$ requires that

\begin{equation} \label{foc_p}
    \begin{split}
        \frac{\partial}{\partial p} \mathcal{L} &= X_t + \beta \left[ \frac{\partial V_{t+1}}{\partial X_{t+1}} \frac{\partial X_{t+1}}{\partial p} + \frac{\partial V_{t+1}}{\partial S_{t+1}} \frac{\partial S_{t+1}}{\partial p}  \right] \\
        &= X_t  + \beta \left[-\gamma \ \frac{\partial V_{t+1}}{\partial X_{t+1}} + \gamma \ \frac{\partial V_{t+1}}{\partial S_{t+1}} \right] \\
        X_t &= \beta \ \gamma \ \left[\frac{\partial V_{t+1}}{\partial X_{t+1}} - \frac{\partial V_{t+1}}{\partial S_{t+1}}  \right]
    \end{split}
\end{equation}

This first order condition implies that the benefit of charging a higher price today, which is today's demand $X_t$, should be equal to the cost of reducing the demand tomorrow, $ \beta \ \gamma \ \left[\frac{\partial V_{t+1}}{\partial X_{t+1}} - \frac{\partial V_{t+1}}{\partial S_{t+1}}  \right] $. The second first order condition requires that, for each neighbor $m$,

\begin{equation} \label{foc_Y}
    \begin{split}
        \frac{\partial}{\partial Y^m} \mathcal{L} &= \sum_{j \in N_{\mathcal{A}}(i)} \frac{\partial (Y^j \  P^j(\Y))}{\partial Y^m} + \lambda = 0.
    \end{split}
\end{equation}

We can distribute the partial derivative by noting that for each entry in the sum, $j$ not equal to $m$, we take the partial derivative of $P^j$ with respect to $Y^m$, and for the entry $m$ in the sum we need to use the chain rule. 

\begin{equation} \label{lambda}
    \begin{split}
        -\lambda &= \sum_{j \in N_{\mathcal{A}}(i)} \frac{\partial (Y^j \  P^j(\Y))}{\partial Y^m} \\
        &= Y^1 \  \frac{\partial P^1(\Y)}{\partial Y^m} + \ldots + \underbrace{\frac{\partial Y^m}{\partial Y^m} P^m(\Y) + Y^m \  \frac{\partial P^m(\Y)}{\partial Y^m} }_{\text{by the chain rule}} + Y^{m+1} \  \frac{\partial P^{m+1}(\Y)}{\partial Y^m} \ldots \\
        &= P^m(\Y) + \sum_{j \in N_{\mathcal{A}}(i)} Y^j \  \frac{\partial P^j(\Y)}{\partial Y^m}
    \end{split}
\end{equation} 

\subsubsection{Envelope}

By the envelope theorem, the first order condition with respect to $X_t$

\begin{equation} \label{envX}
    \begin{split}
        \frac{\partial V_t}{\partial X_t} &= p_t + \lambda_t + \beta \ \frac{\partial V_{t+1}}{\partial X_{t+1} }\frac{\partial X_{t+1}}{\partial X_t} \\
        &= p_t + \lambda_t + \beta \ \frac{\partial V_{t+1}}{\partial X_{t+1} },
    \end{split}
\end{equation}

and

\begin{equation} \label{envS}
    \frac{\partial V_t}{\partial S_t} = \beta \ \frac{\partial V_{t+1}}{\partial S_{t+1}}.
\end{equation}



Then combining Equations (\ref{envX}), (\ref{envS}), and (\ref{foc_p}), we obtain

\begin{equation}
    \begin{split}
         \frac{\partial V_t}{\partial X_t} - \frac{\partial V_t}{\partial S_t} &= \frac{X_t}{\gamma} + p_t + \lambda_t .
    \end{split}
\end{equation}

Iterating forward

\begin{equation}
    \frac{\partial V_{t+1}}{\partial X_{t+1}} - \frac{\partial V_{t+1}}{\partial S_{t+1}} = \frac{X_{t+1}}{\gamma} + p_{t+1} + \lambda_{t+1}.
\end{equation}

Using (\ref{foc_p})

\begin{equation}
    \frac{X_t}{\beta \ \gamma} = \frac{X_t - \B\left[R(p_t, S_t)\right]}{\gamma} + p_{t+1} + \lambda_{t+1}.
\end{equation}

Finally, using the definition $\B\left[R(p_t, S_t)\right]$,

\begin{equation}
    \begin{split}
        \frac{X_t}{\beta \ \gamma} &= \frac{X_t - \alpha - \gamma \  p_t - \eta \ S_t}{\gamma} + p_{t+1} + \lambda_{t+1} \\
        p_{t+1} &= \frac{X_t}{\beta \ \gamma} - \frac{X_t - \alpha - \gamma \  p_t - \eta \ S_t}{\gamma} - \lambda_{t+1} \\
        p_{t+1} &= p_t + \frac{1-\beta}{\beta \ \gamma} X_t + \frac{\alpha + \eta S_t}{\gamma} - \lambda_{t+1}.
    \end{split}
\end{equation} 

\subsubsection{Bargaining solution} \label{a:barsol}

The Nash bargaining solution is such that

\begin{equation*}
    P^{(i, j)} = \arg \max_{P^{(i, j)}} \left\{\Pi_i \  \Pi_j \right\}.
\end{equation*}

The first order condition is such that

\begin{equation*}
    \frac{\partial}{\partial P^{(i, j)}} \left( \Pi_i \  \Pi_j \right) = \frac{\partial \Pi_i}{\partial  P^{(i, j)}} \  \Pi_j + \Pi_i \  \frac{\partial \Pi_j}{\partial  P^{(i, j)}} = 0
\end{equation*}

Then, by symmetry, $\frac{\partial \Pi_j}{\partial  P^{(i, j)}} = - \frac{\partial \Pi_i}{\partial  P^{(i, j)}}$

\begin{equation}
    \Pi_j = \Pi_i .
\end{equation}

Note that an equal division of the pie is consistent with the network bargaining model of \citein{Corominas-Bosch2004} with infinitely patient traders.


\subsection{Jacobian of bargaining price} \label{a:jacobian_p}

Thanks to \citein{greg2020} for pointing this out. I will use

\begin{equation}
    \diag(\Y) P = \diag(P) \Y = \Y \circ P \implies d \ \diag(\Y) P = \diag(P) \ d \Y.
\end{equation}

First let $\matr{D} \coloneqq \diag(\Y)$ and $\matr{A} \coloneqq (2\I + \G)^{-1} \Delta$. Noting that $\frac{d \matr{A}}{d \Y} = 0$

\begin{equation}
    \begin{split}
        P &= \matr{D}^{-1} \matr{A} \\
        dP &= d\matr{D}^{-1} \matr{A} \\
        &= -\matr{D}^{-1} d \matr{D} \underbrace{\matr{D}^{-1} \matr{A}}_{P} \\
        &= -\matr{D}^{-1} \diag(P) \ d \Y.
    \end{split}
\end{equation}

\subsection{Solution to $n$ nodes star graph}

Starting from $Y^2$

\begin{equation*}
    Y^2 = \underbrace{\sum^n_{l = 2} Y^l}_{X_1} \Big/ \underbrace{\sum^n_{l = 2} \frac{Y^l}{Y^2}}_{D}.
\end{equation*}

The denominator can be rewritten as

\begin{equation*}
    \begin{split}
        D &= \sum^n_{l = 2} \frac{\Delta^l - \sum_{m \neq l} \Delta^m / n}{\Delta^2 - \sum_{m \neq 2} \Delta^m / n} \\
        &= \frac{1}{(n-1) \left( \Delta^2 - \sum_{m \neq 2} \frac{\Delta^m}{n} \right)} \left[ \sum^n_{l = 2} \left( \Delta^l - \sum_{m \neq l} \Delta^m / n \right) \right].
    \end{split}
\end{equation*}

The second term can be expanded

\begin{equation*}
    \begin{split}
        \left[ \sum^n_{l = 2} \left( \Delta^l - \sum_{m \neq l} \Delta^m / n \right) \right] &= \sum^n_{l = 2} \Delta^l - \frac{1}{n} \left( \sum_{m \neq 2} \Delta^m + \sum_{m \neq 3} \Delta^m  + \ldots + \sum_{m \neq n} \Delta^m \right) \\
        &= \sum^n_{l = 2} \Delta^l - \frac{n - 2}{n} \sum^n_{m = 2} \Delta^m \\
        &= \frac{2}{n} \sum^n_{l = 2} \Delta^l,
    \end{split}
\end{equation*}

Using the definition of $\Delta^l$,

\begin{equation*}
    \frac{2 \ (n-1)}{n} \underbrace{\left(X_1 p_1 - \frac{1}{n-1} \sum^n_{l = 2} X_l p_l\right)}_{\underline{\Delta}_1}
\end{equation*}

where $\underline{\Delta}_1$ is the average difference of revenue between node $1$ and its neighbors. Going back to $Y^2$,

\begin{equation*}
    Y^2 = \frac{2 \underline{\Delta}_1 X_1}{n  \Delta^2 - \sum_{m \neq 2} \Delta^m }
\end{equation*}

Then

\begin{equation*}
    P^2 = \frac{n \Delta^2 - \sum_{m \neq 2} \Delta^m}{(n-1) Y^2} = \frac{X_1}{2 (n-1) \underline{\Delta}_1}.
\end{equation*}

\subsection{Line graph influence} \label{a:linegraphinfluence}

Consider the case in which the network is line graph with $v$ nodes and $n = v-1$ edges. Then $A$ is a $\R^{v \times v}$ matrix and $\G \in \R^{n \times n}$

\begin{equation}
    \begin{split}
        A_{i, j} = A_{j, i} &= \begin{cases}
            1 & \text{if } i = j - 1 \\
            0 & \text{otherwise}
        \end{cases} \\
        \G_{i, j} &= \begin{cases}
            -1 & \text{if } i = j + 1, i = j - 1 \\
            0  & \text{otherwise}
        \end{cases}
    \end{split}
\end{equation}

Note that this implies that

\begin{equation}
    \matr{2I + G} =
    \begin{pmatrix}
        {2} & {-1} & {}       & {}       & {}   \\
        {-1} & {2}  & {-1}     & {}       & {}   \\
        {}  & {-1}  & {\ddots} & {\ddots} & {}   \\
        {}  & {}   & {\ddots} & {\ddots} & {-1} \\
        {}  & {}   & {}       & {-1}      & {2}
    \end{pmatrix}
\end{equation}

is a symmetric tridiagonal matrix. This class of matrix can be inverted as in \citein{Huang1997}. First, notice

\begin{equation}
    (\matr{2I + \G})^{-1} = 2^{-1} \left(\I + \frac{1}{2} \G\right)^{-1} \eqqcolon 2^{-1} (\T_n)^{-1}
\end{equation}

where

\begin{equation}
    \T_n = \begin{pmatrix}
        {1} & {-\frac{1}{2}} & {}       & {}       & {}   \\
        {-\frac{1}{2}} & {1}  & {-\frac{1}{2}}     & {}       & {}   \\
        {}  & {-\frac{1}{2}}  & {\ddots} & {\ddots} & {}   \\
        {}  & {}   & {\ddots} & {\ddots} & {-\frac{1}{2}} \\
        {}  & {}   & {}       & {-\frac{1}{2}}      & {1}
    \end{pmatrix}_{n \times n}.
\end{equation}

In order to find $(\T_n)^{-1}$ we first focus on $\det(\T_n)$. Using the Laplace expansion of the determinant over the first row we can write

\begin{equation*}
    \det(\T_n) = \underbrace{\det \begin{pmatrix}
        {1} & {-\frac{1}{2}} & {}       & {}       & {}   \\
        {-\frac{1}{2}} & {1}  & {-\frac{1}{2}}     & {}       & {}   \\
        {}  & {-\frac{1}{2}}  & {\ddots} & {\ddots} & {}   \\
        {}  & {}   & {\ddots} & {\ddots} & {-\frac{1}{2}} \\
        {}  & {}   & {}       & {-\frac{1}{2}}      & {1}
    \end{pmatrix}_{n-1 \times n-1}}_{\det(\T_{n-1})}
    + \frac{1}{2} \det \begin{pmatrix}
        {-\frac{1}{2}} & {-\frac{1}{2}} & {}       & {}       & {}   \\
        {0} & {1}  & {-\frac{1}{2}}     & {}       & {}   \\
        {}  & {-\frac{1}{2}}  & {\ddots} & {\ddots} & {}   \\
        {}  & {}   & {\ddots} & {\ddots} & {-\frac{1}{2}} \\
        {}  & {}   & {}       & {-\frac{1}{2}}      & {1}
    \end{pmatrix}_{n-1 \times n-1}. 
\end{equation*}

We can further take a Laplace expansion of the second entry to get

\begin{equation*}
    \det \begin{pmatrix}
        {-\frac{1}{2}} & {-\frac{1}{2}} & {}       & {}       & {}   \\
        {0} & {1}  & {-\frac{1}{2}}     & {}       & {}   \\
        {}  & {-\frac{1}{2}}  & {\ddots} & {\ddots} & {}   \\
        {}  & {}   & {\ddots} & {\ddots} & {-\frac{1}{2}} \\
        {}  & {}   & {}       & {-\frac{1}{2}}      & {1}
    \end{pmatrix}_{n-1 \times n-1} = - \frac{1}{2} \underbrace{\det \begin{pmatrix}
        {1} & {-\frac{1}{2}} & {}       & {}       & {}   \\
        {-\frac{1}{2}} & {1}  & {-\frac{1}{2}}     & {}       & {}   \\
        {}  & {-\frac{1}{2}}  & {\ddots} & {\ddots} & {}   \\
        {}  & {}   & {\ddots} & {\ddots} & {-\frac{1}{2}} \\
        {}  & {}   & {}       & {-\frac{1}{2}}      & {1}
    \end{pmatrix}_{n-2 \times n-2}}_{\det(\T_{n-2})}.
\end{equation*}

This determines a linear difference equation of the form

\begin{equation*}
    \det(\T_n) = \det(\T_{n-1}) - \frac{1}{4} \det(\T_{n-2})
\end{equation*}

with initial conditions

\begin{equation*}
    \begin{split}
        \det(\T_1) &= 1 \\
        \det(\T_2) &= \det \begin{pmatrix}
            1 & -\frac{1}{2} \\
            -\frac{1}{2} & 1
        \end{pmatrix} = \frac{3}{4}
    \end{split}
\end{equation*}

which gives the sequence

\begin{equation*}
    \{\det(\T)\}_n = \left\{1, \frac{3}{4}, \frac{1}{2}, \frac{5}{16} \ldots \right\}.
\end{equation*}


This is a well known sequence (\cite{weisstein2021}) with solution 

\begin{equation*}
    \det(\T_n) = \frac{n+1}{2^n}.
\end{equation*}

which also implies that

\begin{equation}
    \begin{split}
        \det(\T_n) \det(\T_m) &= \frac{(n+1)(m+1)}{2^{n+m}} \\
        \det(\T_n) / \det(\T_m) &= \frac{(n+1) / (m+1)}{2^{n-m}}
    \end{split}.
\end{equation}


By writing the inverse using Cramer's rule we know that

\begin{equation}
    (\T_n^{-1})_{i, j} = \frac{(-1)^{i + j}}{\det(\T_n)} (M_n)_{i, j} 
\end{equation}

where $(M_n)_{i, j} $ is the determinant of the matrix obtained by deleting row $i$ and column $j$. Consider first the case of $i = j$. It can be shown that

\begin{equation*}
    \begin{split}
        (M_n)_{i, i} = \det \begin{pmatrix}
            \T_{i - 1} & 0 \\
            0 & \T_{n-i}
        \end{pmatrix} &= \det(\T_{i - 1}) \det(\T_{n - i}) .
    \end{split}
\end{equation*}

This gives

\begin{equation}
    \begin{split}
        (\T_n^{-1})_{i, i} &= (-1)^{2i} \frac{\det(\T_{i - 1}) \det(\T_{n - i}) }{\det(\T_{n})} \\
        &= 2 \frac{i(n - i + 1)}{n + 1} = 2\left( i - \frac{i^2}{n + 1} \right).
    \end{split}
\end{equation}

This can be generalized for $i \geq j$. The symmetric case follows since since $(M_n)_{i, j} = (M_n)_{j, i}$. Non trivially, the minor associated with $(M_n)_{i, j}$, say $(\matr{M}_n)_{i, j}$ is a block matrix with the structure below.

\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        % Init
        \multicolumn{2}{|c|}{\multirow{2}{*}{$\T_{i - 1}$}}  & \multicolumn{6}{c|}{}  \\[8pt]
        \cline{3-3}
        \multicolumn{2}{|c|}{}  & $-1/2$  & \multicolumn{5}{c|}{}  \\[8pt]
        \cline{1-5}
        \multicolumn{2}{|c|}{}  & $-1/2$  & \multicolumn{2}{|c|}{\multirow{2}{*}{$\T_{j - i - 1}$}} &  \multicolumn{3}{c|}{} \\[8pt]
        \cline{3-3} \cline{6-6}
        \multicolumn{3}{|c}{}  & \multicolumn{2}{|c|}{} & $-1/2$ &  \multicolumn{2}{|c|}{} \\[8pt]
        \cline{4-8}
        \multicolumn{3}{|c}{} & \multicolumn{2}{c|}{} & $-1/2$ & \multicolumn{2}{|c|}{\multirow{2}{*}{$\T_{n-j}$}} \\[8pt]
        \cline{6-6}  
        \multicolumn{6}{|c}{} & \multicolumn{2}{|c|}{}\\[8pt]
        % End
        \hline
    \end{tabular}
\end{center}

Taking the cofactor over the first row gives, for $j > i$

\begin{equation}
    (M_n)_{i, j} = (M_{n-1})_{i - 1, j-1} - \frac{1}{4}  (M_{n-2})_{i - 2, j-2} .
\end{equation}

This equation, after a bit of manipulation leads to the following expression

\begin{equation}
    (M_n)_{i, j} = \frac{(-1)^{2(n-1) - (i+j)}}{2 (j - i)} \det(\T_{j-i-1}) \det(\T_{n-j}) \det(\T_{i-1})
\end{equation}

Using again this to find the determinant yields

\begin{equation*}
    \begin{split}
        (\T_n^{-1})_{i, j} &=  \frac{(-1)^{i+j}}{\det(\T_n)} (-1)^{2(n-1) - (i+j)} 
        \frac{\det(\T_{j-i-1}) \det(\T_{n-j}) \det(\T_{i-1})}{2 (j - i)}\\
        &= \frac{1}{2 (j - i)} \frac{\det(\T_{j-i-1}) \det(\T_{n-j}) \det(\T_{i-1})}{\det(\T_n)} \\
        &= \frac{1}{2 (j - i)}  \frac{(j-i) (n - j + 1) (i)}{(n + 1)} \frac{2^n}{2^{n - 2}} \\
        &= 2 \frac{(n - j + 1) (i)}{n + 1} = 2\left(i - \frac{j i}{n+1}\right)
    \end{split}
\end{equation*}

Note that this result is consistent with $j = i$. Finally, we want to return to our original Bargaining power matrix

\begin{equation}
    \begin{split}
        (\matr{2I + \G})^{-1}_{i, j} &= 2^{-1} (\T_n)^{-1}_{i, j} \\
        &=i - \frac{j i}{n+1}.
    \end{split}
\end{equation}

Letting the lower triangular operator $\matr{L}(\matr{A})$, we can rewrite

\begin{equation}
    \begin{split}
        (\matr{2I + \G})^{-1} &= \matr{L}(\matr{\iota \iota}^T) \left(\I - \frac{1}{n+1} \matr{\iota \iota}^T\right) \matr{L}(\matr{\iota \iota}^T)
    \end{split}
\end{equation}

\subsection{Policy function of central node in path} \label{a:pol_path}

Note that using the vector of revenue differences $\Delta$, we can write

\begin{equation}
    \begin{split}
        (\matr{2I + \G})^{-1}_{i} \Delta &= \sum^n_{j = 1}\left(\min\{i, j\} - \frac{i j}{n+1} \right) \Delta^j\\
        &= \left( 1 -\frac{i 1}{n+1} \right) \sum_{j < i} j \Delta^j + i \sum_{j \geq i} \left(1 - \frac{j}{n+1} \right) \Delta^j
    \end{split}
\end{equation}

Assume $n$ is even and consider the central node $m = (n+1) / 2$. The edge $(m-1, m)$ is the $(m-1)$-th. The ``left'' revenue is then

\begin{equation*}
    \begin{split}
        P^{m-1} Y^{m-1} &= \left( 1 - \frac{m - 1}{n+1} \right) \sum_{j < m-1} j \Delta^{j} + (m-1)\sum_{j \geq m-1} \left(1 - \frac{j}{n+1}\right) \Delta^{j}\\
        &= \left(\frac{1}{2} + \frac{1}{n+1}\right) \sum_{j < m-1} j \Delta^{j}+ \frac{n-1}{2} \sum_{j \geq m-1} \left(1 - \frac{j}{n+1}\right) \Delta^{j}\\
        &= \left(\frac{1}{2} + \frac{1}{n+1}\right) \sum_{j < m-1} j \Delta^{j} + \frac{n-1}{2} \sum_{j > m} \left(1 - \frac{j}{n+1}\right) \Delta^{j} + \\
        &+  \frac{n-1}{2} \left( \frac{1}{2} + \frac{1}{n+1} \right) \Delta^{m-1} + \frac{n+1}{4} \Delta^m
    \end{split}
\end{equation*}

and the ``right'' revenue is

\begin{equation*}
    \begin{split}
        P^{m} Y^m &= \frac{1}{2} \sum_{j < m} j \Delta^{j} + \frac{n+1}{2}\sum_{j \geq m} \left(1 - \frac{j}{n+1}\right) \Delta^{j} \\
        &= \frac{1}{2} \sum_{j < m-1} j \Delta^{j} + \frac{1}{2} (m-1)\Delta^{m-1} +\frac{n+1}{2} \left(1 - \frac{m}{n+1}\right) \Delta^m \\
        &+ \frac{n+1}{2} \sum_{j > m} \left(1 - \frac{j}{n+1}\right) \Delta^{j} \\
        &= \frac{1}{2} \sum_{j < m-1} j \Delta^{j} + \frac{n-1}{4} \Delta^{m-1} +\frac{n+1}{4} \Delta^m + \frac{n+1}{2} \sum_{j > m} \left(1 - \frac{j}{n+1}\right) \Delta^{j} \\
    \end{split}
\end{equation*}

Using price equalization we know that

\begin{equation}
    \begin{split}
        Y_r(\Delta) \coloneqq \frac{P^m Y^m}{P^{m-1} Y^{m-1}} &= \frac{Y^m}{Y^{m-1}}\\
        &= \frac{
            \frac{1}{2} \sum_{j < m} j \Delta^{j} + \frac{n+1}{2}\sum_{j \geq m} \left(1 - \frac{j}{n+1}\right) \Delta^{j}
        }{
            \left(\frac{1}{2} + \frac{1}{n+1}\right) \sum_{j < m-1} j \Delta^{j}+ \frac{n-1}{2} \sum_{j \geq m-1} \left(1 - \frac{j}{n+1}\right) \Delta^{j}
        }
    \end{split}
\end{equation}


Then we can use the constraint $X_m = Y^{m-1} + Y^m$ and the ratio $Y_r$ to notice that

\begin{equation}
    \begin{split}
        Y^{m} = \frac{X_m}{1 + 1 / Y_r(\Delta)}
    \end{split}
\end{equation}

hence,

\begin{equation}
    2 P^m(X_m, p_m, \Delta) = \left( \frac{1 + 1 / Y_r(\Delta)}{X_m} \right) \left( \sum_{j < m} j \Delta^{j} + (n+1) \sum_{j \geq m} \left(1 - \frac{j}{n+1}\right) \Delta^{j} \right)
\end{equation}

\subsection{Convergence of beliefs} \label{a:beliefconv}

Starting from the interior definition of $r$

\begin{equation}
    r(s_t, p_t) =  \frac{1 - \beta}{\beta} \ s_t - \sqrt{ \left(\frac{1 - \beta}{\beta} \ s_t \right)^2 - \ \left(p_t - k\right) }
\end{equation}

The two first order conditions are

\begin{equation}
    \frac{\partial r}{\partial p_t} = \frac{1}{\sqrt{ \left(\frac{1 - \beta}{\beta} \ s_t \right)^2 - \ \left(p_t - k\right) }},
\end{equation}

and 

\begin{equation}
    \frac{\partial r}{\partial s_t} = \frac{1 - \beta}{\beta} - \frac{2 \left( \frac{1 - \beta}{\beta}\right)^2 s_t}{\sqrt{ \left(\frac{1 - \beta}{\beta} \ s_t \right)^2 - \ \left(p_t - k\right) }}.
\end{equation}

As an indicative proxy for the convergence of the beliefs we can compute the Taylor expansion about $(s_t, p_t) = (s_0, k)$. This gives

\begin{equation}
    r(s_t, p_t) \approx \frac{\beta}{2 (1- \beta) s_0} (p_t - k)
\end{equation}

This implies that we expect, in absence of shocks, if $p_t \approx k$ and $S_t$ is approximately constant

\begin{equation}
    \begin{split}
        \alpha_t \xrightarrow{t \rightarrow \infty} \ \ &\frac{-k \beta}{2 (1-\beta) s_0} \\
        \eta_t \xrightarrow{t \rightarrow \infty} \ \ &0\\
        \gamma_t \xrightarrow{t \rightarrow \infty} \ \ & \frac{\beta}{2 (1-\beta) s_0} p_t
    \end{split}
\end{equation}

We can see this happening by plotting the behavior of $\alpha_t$, $\eta_t$, and $\gamma_t$ in a simulation without shocks and interactions.


\begin{figure}[H]
    \centering
    \includegraphics[width = \textwidth]{\plotpath/constant/star/ols.pdf}
    \caption{Convergence of beliefs}
    \label{fig:ols_convergence}
\end{figure}
