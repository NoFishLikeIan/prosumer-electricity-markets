\documentclass[../thesis.tex]{subfiles}

\section{Optimization solutions}

\subsection{Producer optimization} \label{a:producer_optimization}

Let,

\begin{equation}
    J(s, p, r) = s \  (p - k) - c(s, r) \  r + \beta \  V(s + r, p)
\end{equation}

such that, Equation (\ref{bellman_prod}) can be written as,

\begin{equation}
    V(s, p) = \max_{r_t \in [\underline{r}, \bar{r}]} J(s, p, r).
\end{equation}

Maximization of $J$ requires (first order condition),

\begin{equation} \label{J_foc_r}
    \frac{\partial J}{\partial r}(s, p, r)  = -\frac{\partial c}{\partial r}(s, r) \  r - c(s, r) + \beta \  \frac{\partial V}{\partial s}(s + r, p) = 0
\end{equation}

Assuming maximization in $r$, by the envelope theorem,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= (p - k) - \frac{\partial c}{\partial s}(s, r) \  r + \beta \  \frac{\partial V}{\partial s}(s + r, p)
    \end{split}
\end{equation}

Using Equation (\ref{J_foc_r}) on the first condition yields,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= (p - k) - \frac{\partial c}{\partial s}(s, r) \  r + \frac{\partial c}{\partial r}(s, r) \  r + c(s, r) \\
        &= (p - k) + \left[\frac{\partial c}{\partial r}(s, r) - \frac{\partial c}{\partial s}(s, r)\right] \  r + c(s, r)
    \end{split}
\end{equation}

Iterating forward yields,

\begin{equation}
    \frac{\partial V}{\partial s}(s + r, p)  = p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \  r + c(s + r, r)
\end{equation}

Using (\ref{J_foc_r}),

\begin{equation}
    \frac{\frac{\partial c}{\partial r}(s, r) \  r + c(s, r)}{\beta} =  p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \  r + c(s + r, r)
\end{equation}

\subsection{Stability of different cost functions}

The first order condition for $r$ requires that,

\begin{equation}
    \frac{\frac{\partial c}{\partial r}(s, r) \  r + c(s, r)}{\beta} =  p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \  r + c(s + r, r)
\end{equation}

Any cost function with the trivial property that not ramping up is free, $c(s, 0) = 0$, is stable, $r = 0$, if and only if,


\begin{equation}
    p = k
\end{equation}

\subsection{Limiting behaviour as $c_1 \xrightarrow{} \infty$}\label{a:limiting}
\newcommand{\limc}{\lim_{c_1 \xrightarrow{} \infty}}

Assume a softplus cost function and notice that, letting

\begin{equation*}
    \sigm_{c_1}(x) = \frac{1}{1 + \exp(-c_1 \  x)},
\end{equation*}

we can write,

\begin{equation}
    \frac{\partial c}{\partial s}(s, r) = \sigm_{c_1}(s \  r) \  r \ \text{ and } \ \frac{\partial c}{\partial r}(s, r) = \sigm_{c_1}(s \  r) \  s
\end{equation}


Hence the two function that regulate the first order condition of the prosumer can be rewritten as,

\begin{equation}
    \begin{split}
        mc(r) &= \frac{\partial c}{\partial r}(s_t, r) \  r + c(s_t, r) \\
        &= \sigm_{c_1}(s_t \  r) \  s_t \  r_t + c(s_t, r)
    \end{split}
\end{equation}

and

\begin{equation}
    \begin{split}
        mb(r) &= p_t - k + \left[\frac{\partial c}{\partial r}(s_t + r, r) - \frac{\partial c}{\partial s_t}(s_t + r, r)\right] \  r + c(s_t + r, r)\\
        &= p_t - k + \left[\sigm_{c_1}(.) \  (s_t + r) - \sigm_{c_1}(.) \  r \right] \  r + c(s_t + r, r) \\
        &=p_t - k + \sigm_{c_1}((s_t + r) \  r) \  s_t \  r + c(s_t + r, r)
    \end{split}
\end{equation}

We can then study the behaviour of the two functions as $c_1 \xrightarrow{} \infty$. Given our model we can assume $s_t \geq 0$. First, notice that,

\begin{equation}
    \limc \sigm_{c_1}(x) = \begin{cases}
        1 & \text{if } x \geq 0 \\
        0 & \text{if } x < 0
    \end{cases}
\end{equation}

We can use this and the fact that $\limc c(s, r) = \max\{0, s \  r\}$, to evaluate,

\begin{equation}
    \limc mc(r) = \max \{0, 2s_t \  r\}
\end{equation}

and,

\begin{equation*}
    \limc mb(r) = p_t - k + \max\{0, (s_t + r) \  r\} + \begin{cases}
        s_t \  r &\text{if } r \in (-\infty, -s_t] \cup [0, \infty) \\
        0 &\text{if } r \in (-s_t, 0)
    \end{cases}
\end{equation*}

Using the optimization constraint, $r \in [-s_t, \infty)$, we obtain,

\begin{equation}
    \limc mb(r) = p_t - k + \begin{cases}
        2 s_t \  r + r^2 &\text{if } r > 0 \\
        0 &\text{if } r \in [-s_t, 0]
    \end{cases}
\end{equation}

Then, in the limit, we can rewrite the first order condition as,

\begin{equation*}
    \begin{split}
        \limc mc(r) &= \beta \limc mb(r) \\
        \frac{1}{\beta} \begin{cases}
            2 s_t \  r  &\text{if } r > 0 \\
            0 &\text{if } r \in [-s_t, 0]
        \end{cases} &= p_t - k  + \begin{cases}
            2 s_t \  r + r^2 &\text{if } r > 0 \\
            0 &\text{if } r \in [-s_t, 0]
        \end{cases}
    \end{split}
\end{equation*}

This allows us to find an approximate policy function. Namely, assuming producing is profitable, $p_t - k > 0$ and $s_t  > \frac{\beta}{1-\beta} \sqrt{p_t - k}$, we have the interior solution,

\begin{equation}
    r(s_t, p_t) = \frac{1}{2} \  \left[  \left(\frac{1 - \beta}{\beta} \  2 s_t \right) - \sqrt{\left(\frac{1 - \beta}{\beta} \  2 s_t \right)^2 - 4 \  \left(p_t - k\right)} \right].
\end{equation}

If, on the other hand, $s_t \leq \frac{\beta}{1-\beta} \sqrt{p_t - k}$, then $\beta \  mb(r) > mc(r)$, hence,

\begin{equation}
    r(s_t, p_t) = \frac{\beta}{1-\beta} \sqrt{p_t - k} - s_t
\end{equation}

Finally, if $p_t - k < 0$

\begin{equation}
    r(s_t, p_t) = -\gamma \  s
\end{equation}

\subsection{Evolution of excess demand} \label{a:ev_demand}

Using the definition of $X_t$, equation (\ref{x_inst}), we can rewrite,

\begin{equation}
    \begin{split}
        X_{t+1} &= M \  e_{t+1} - S_{t+1} \\
        S_{t+1} &= M \  e_{t+1} - X_{t+1} = R_t + S_t
    \end{split}
\end{equation}

hence,

\begin{equation}
    \begin{split}
        X_{t+1} - M \  e_{t+1} &= X_t - M \  e_t - R_t \\
        X_{t+1} &= X_t + M \  \left(e_{t+1} - e_t \right) -  R_t
    \end{split}
\end{equation}

\subsection{Provider optimization} \label{a:provider_optimization}

\subsubsection{Euler equation}

First assume that $X_t > 0$. The case for $X_t < 0$ is perfectly symmetric. Let the approximated ramp-up function be,

\begin{equation}
    \B[R(p_t, S_t)] = \alpha + \gamma \  p_t + \eta \ S_t.
\end{equation}

Then let,

\begin{equation}
    \begin{split}
        X_{t+1} &= X_t - \B[R(p_t, S_t)] \\
        S_{t+1} &= S_t + \B[R(p_t, S_t)] \\
        V_{t+1} &= V(X_{t+1}, S_{t+1})
    \end{split}
\end{equation}

Each period the provider faces the problem,

\begin{equation*}
    \begin{split}
        \mathcal{L}(p, \Y, X_t, S_t) &= p \  X_t - \sum_{j \in N_{\mathcal{A}}(i)} Y^j \  P^j(\Y) \\
        &+ \lambda_t \  \left( X_t -  \sum_{j} Y^j \right) + \beta \ V_{t+1}
    \end{split}
\end{equation*}

such that $V(X_t, S_t) = \max_{p, \Y} \mathcal{L}(p, \Y, X_t, S_t)$. The first order condition of $\mathcal{L}$ requires that,

\begin{equation} \label{foc_p}
    \begin{split}
        \frac{\partial}{\partial p} \mathcal{L} &= X_t + \beta \left[ \frac{\partial V_{t+1}}{\partial X_{t+1}} \frac{\partial X_{t+1}}{\partial p} + \frac{\partial V_{t+1}}{\partial S_{t+1}} \frac{\partial S_{t+1}}{\partial p}  \right] \\
        &= X_t  + \beta \left[-\gamma \ \frac{\partial V_{t+1}}{\partial X_{t+1}} + \gamma \ \frac{\partial V_{t+1}}{\partial S_{t+1}} \right] \\
        X_t &= \beta \ \gamma \ \left[\frac{\partial V_{t+1}}{\partial X_{t+1}} - \frac{\partial V_{t+1}}{\partial S_{t+1}}  \right]
    \end{split}
\end{equation}

This first order condition implies that the benefit of charging a higher price today, which is today's demand $X_t$, should be equal to the cost of reducing the demand tomorrow, $ \beta \ \gamma \ \left[\frac{\partial V_{t+1}}{\partial X_{t+1}} - \frac{\partial V_{t+1}}{\partial S_{t+1}}  \right] $. The second first order condition requires that, for each neighbor $m$, 

\begin{equation} \label{foc_Y}
    \begin{split}
        \frac{\partial}{\partial Y^m} \mathcal{L} &= \sum_{j \in N_{\mathcal{A}}(i)} \frac{\partial (Y^j \  P^j(\Y))}{\partial Y^m} + \lambda = 0
    \end{split}
\end{equation}

We can distribute the partial derivative by noting that for each entry in the sum, $j$ not equal to $m$, we take the partial derivative of $P^j$ with respect to $Y^m$, and for the entry $m$ in the sum we need to use the chain rule. 

\begin{equation} \label{lambda}
    \begin{split}
        -\lambda &= \sum_{j \in N_{\mathcal{A}}(i)} \frac{\partial (Y^j \  P^j(\Y))}{\partial Y^m} \\
        &= Y^1 \  \frac{\partial P^1(\Y)}{\partial Y^m} + \ldots + \underbrace{\frac{\partial Y^m}{\partial Y^m} P^m(\Y) + Y^m \  \frac{\partial P^m(\Y)}{\partial Y^m} }_{\text{by the chain rule}} + Y^{m+1} \  \frac{\partial P^{m+1}(\Y)}{\partial Y^m} \ldots \\
        &= P^m(\Y) + \sum_{j \in N_{\mathcal{A}}(i)} Y^j \  \frac{\partial P^j(\Y)}{\partial Y^m}
    \end{split}
\end{equation} 

\subsubsection{Envelope}

By the envelope theorem, the first order condition with respect to $X_t$

\begin{equation} \label{envX}
    \begin{split}
        \frac{\partial V_t}{\partial X_t} &= p_t + \lambda_t + \beta \ \frac{\partial V_{t+1}}{\partial X_{t+1} }\frac{\partial X_{t+1}}{\partial X_t} \\
        &= p_t + \lambda_t + \beta \ \frac{\partial V_{t+1}}{\partial X_{t+1} },
    \end{split}
\end{equation}

and

\begin{equation} \label{envS}
    \frac{\partial V_t}{\partial S_t} = \beta \ \frac{\partial V_{t+1}}{\partial S_{t+1}}.
\end{equation}



Then combining Equations (\ref{envX}), (\ref{envS}), and (\ref{foc_p}), we obtain

\begin{equation}
    \begin{split}
         \frac{\partial V_t}{\partial X_t} - \frac{\partial V_t}{\partial S_t} &= \frac{X_t}{\gamma} + p_t + \lambda_t 
    \end{split}
\end{equation}

Iterating forward,

\begin{equation}
    \frac{\partial V_{t+1}}{\partial X_{t+1}} - \frac{\partial V_{t+1}}{\partial S_{t+1}} = \frac{X_{t+1}}{\gamma} + p_{t+1} + \lambda_{t+1} 
\end{equation}

Using (\ref{foc_p}),

\begin{equation}
    \frac{X_t}{\beta \ \gamma} = \frac{X_t - \B\left[R(p_t, S_t)\right]}{\gamma} + p_{t+1} + \lambda_{t+1}
\end{equation}

Finally, using the definition $\B\left[R(p_t, S_t)\right]$,

\begin{equation}
    \begin{split}
        \frac{X_t}{\beta \ \gamma} &= \frac{X_t - \alpha - \gamma \  p_t - \eta \ S_t}{\gamma} + p_{t+1} + \lambda_{t+1} \\
        p_{t+1} &= \frac{X_t}{\beta \ \gamma} - \frac{X_t - \alpha - \gamma \  p_t - \eta \ S_t}{\gamma} - \lambda_{t+1} \\
        p_{t+1} &= p_t + \frac{1-\beta}{\beta \ \gamma} X_t + \frac{\alpha + \eta S_t}{\gamma} - \lambda_{t+1}
    \end{split}
\end{equation} 

\subsubsection{Bargaining solution} \label{a:barsol}

The Nash bargaining solution is such that

\begin{equation*}
    P^{(i, j)} = \arg \max_{P^{(i, j)}} \left\{\Pi_i \  \Pi_j \right\}.
\end{equation*}

The first order condition is such that,

\begin{equation*}
    \frac{\partial}{\partial P^{(i, j)}} \left( \Pi_i \  \Pi_j \right) = \frac{\partial \Pi_i}{\partial  P^{(i, j)}} \  \Pi_j + \Pi_i \  \frac{\partial \Pi_j}{\partial  P^{(i, j)}} = 0
\end{equation*}

Then, by symmetry, $\frac{\partial \Pi_j}{\partial  P^{(i, j)}} = - \frac{\partial \Pi_i}{\partial  P^{(i, j)}}$,

\begin{equation}
    \Pi_j = \Pi_i 
\end{equation}

Note that an equal division of the pie is consistent with the network bargaining model of \citein{Corominas-Bosch2004} with infinitely patient traders.


\subsection{Jacobian of bargaining price} \label{a:jacobian_p}

Thanks to \citein{greg2020} for pointing this out. I will use,

\begin{equation}
    \diag(\Y) P = \diag(P) \Y = \Y \circ P \implies d \ \diag(\Y) P = \diag(P) \ d \Y
\end{equation}

First let $\matr{D} \coloneqq \diag(\Y)$ and $\matr{A} \coloneqq (2\I + \G)^{-1} \Delta$. Noting that $\frac{d \matr{A}}{d \Y} = 0$,

\begin{equation}
    \begin{split}
        P &= \matr{D}^{-1} \matr{A} \\
        dP &= d\matr{D}^{-1} \matr{A} \\
        &= -\matr{D}^{-1} d \matr{D} \underbrace{\matr{D}^{-1} \matr{A}}_{P} \\
        &= -\matr{D}^{-1} \diag(P) \ d \Y
    \end{split}
\end{equation}

\subsection{Solution to $n$ nodes star graph}

Starting from $Y^2$

\begin{equation*}
    Y^2 = \underbrace{\sum^n_{l = 2} Y^l}_{X_1} \Big/ \underbrace{\sum^n_{l = 2} \frac{Y^l}{Y^2}}_{D}
\end{equation*}

The denominator can be rewritten as,

\begin{equation*}
    \begin{split}
        D &= \sum^n_{l = 2} \frac{\Delta^l - \sum_{m \neq l} \Delta^m / n}{\Delta^2 - \sum_{m \neq 2} \Delta^m / n} \\
        &= \frac{1}{(n-1) \left( \Delta^2 - \sum_{m \neq 2} \frac{\Delta^m}{n} \right)} \left[ \sum^n_{l = 2} \left( \Delta^l - \sum_{m \neq l} \Delta^m / n \right) \right]
    \end{split}
\end{equation*}

The second term can be expanded,

\begin{equation*}
    \begin{split}
        \left[ \sum^n_{l = 2} \left( \Delta^l - \sum_{m \neq l} \Delta^m / n \right) \right] &= \sum^n_{l = 2} \Delta^l - \frac{1}{n} \left( \sum_{m \neq 2} \Delta^m + \sum_{m \neq 3} \Delta^m  + \ldots + \sum_{m \neq n} \Delta^m \right) \\
        &= \sum^n_{l = 2} \Delta^l - \frac{n - 2}{n} \sum^n_{m = 2} \Delta^m \\
        &= \frac{2}{n} \sum^n_{l = 2} \Delta^l,
    \end{split}
\end{equation*}

Using the definition of $\Delta^l$,

\begin{equation*}
    \frac{2 \ (n-1)}{n} \underbrace{\left(X_1 p_1 - \frac{1}{n-1} \sum^n_{l = 2} X_l p_l\right)}_{\underline{\Delta}_1}
\end{equation*}

where $\underline{\Delta}_1$ is the average difference of revenue between node $1$ and its neighbors. Going back to $Y^2$,

\begin{equation*}
    Y^2 = \frac{2 \underline{\Delta}_1 X_1}{n  \Delta^2 - \sum_{m \neq 2} \Delta^m }
\end{equation*}

Then,

\begin{equation*}
    P^2 = \frac{n \Delta^2 - \sum_{m \neq 2} \Delta^m}{(n-1) Y^2} = \frac{X_1}{2 (n-1) \underline{\Delta}_1}
\end{equation*}

\subsection{Line graph influence}

Consider the case in which the network is line graph with $v$ nodes. Then $A$ is a $\R^{v \times v}$ matrix and $\G$

\begin{equation}
    \begin{split}
        A_{i, j} = A_{j, i} &= \begin{cases}
            1 & \text{if } i = j - 1 \\
            0 & \text{otherwise}
        \end{cases} \\
        \G_{i, j} &= \begin{cases}
            1  & \text{if } i = j - 1 \\
            -1 & \text{if } i = j + 1 \\
            0  & \text{otherwise}
        \end{cases}
    \end{split}
\end{equation}

Note that this implies that,

\begin{equation}
    \matr{2I + G} =
    \begin{pmatrix}
        {2} & {-1} & {}       & {}       & {}   \\
        {1} & {2}  & {-1}     & {}       & {}   \\
        {}  & {1}  & {\ddots} & {\ddots} & {}   \\
        {}  & {}   & {\ddots} & {\ddots} & {-1} \\
        {}  & {}   & {}       & {1}      & {2}
    \end{pmatrix}
\end{equation}

is a tridiagonal Toeplitz matrix. This can be inverted using \citein{Huang1997},

\begin{equation}
    (\matr{2I + G})^{-1}_{i, j} = \begin{cases}
        (-1)^{i + j} \phi_{j + 1} \frac{\theta_{i - 1}}{\theta_n} & \text{if } i \leq j \\
        (-1)^{2i } \phi_{i + 1} \frac{\theta_{j - 1}}{\theta_n}   & \text{if } i > j
    \end{cases}
\end{equation}

Where,

\begin{equation}
    \begin{split}
        \phi_i &= 2\phi_{i + 1} + \phi_{i + 2} \ \text{ with } \ \phi_{n+1} = 1, \phi_n = 2 \\
        \theta_i &= 2\theta_{i - 1} + \theta_{i - 2} \ \text{ with } \ \theta_{0} = 1, \theta_1 = 2
    \end{split}
\end{equation}


$\theta_i$ is the sequence of shifted Pell numbers $P_{i+1}$ that have a closed form solution. Likewise, $\phi_i$ is the sequence of shifted Penn number $P_{n - i + 2}$. Hence we can rewrite,

\begin{equation}
    (\matr{2I + G})^{-1}_{i, j} = \begin{cases}
        (-1)^{i + j} \  P_{n + 1- j} \  P_i / P_{n+1} & \text{if } i \leq j \\
        (-1)^{2i } \  P_{n + 1- i} \  P_j / P_{n+1}   & \text{if } i > j
    \end{cases}
\end{equation}

Letting,

\begin{equation*}
    H(i, j, n) \coloneqq \frac{P_{n + 1 - j} \  P_i}{P_{n+1}}
\end{equation*}

we can rewrite,

\begin{equation}
    (\matr{2I + G})^{-1}_{i, j} = \begin{cases}
        (-1)^{i + j} H(i, j, n) & \text{if } i \leq j \\
        (-1)^{2i } H(j, i, n)   & \text{if } i > j
    \end{cases}
\end{equation}

Let the silver ratios be,

\begin{equation}
    \begin{split}
        \delta_{p} &= 1 + \sqrt{2} \\
        \delta_{m} &= 1 - \sqrt{2}
    \end{split}
\end{equation}

Using the properties, $P_{-a} = (-1)^{a+1} P_a$, $P_{a+b} = P_{a} P_{b+1} + P_{a-1} P_b$, and some manipulation

\begin{equation}
    \begin{split}
        \lim_{n \xrightarrow{} \infty} H(i, j, n) &= (-1)^{j + 1}  \left(P_j \  \delta_{p} - P_{j+1} \right) P_i \\
        &= \frac{(-1)^j }{2\sqrt{2}} \  (\delta_{p}^i - \delta_{m}^i) \  \delta^j_{m}
    \end{split}
\end{equation}

\subsection{Initial condition of simulation} \label{a:initsim}

\subsection{Convergence of beliefs} \label{a:beliefconv}