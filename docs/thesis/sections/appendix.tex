\documentclass[../main.tex]{subfiles}

\section{Optimization solutions}

\subsection{Producer optimization} \label{a:producer_optimization}

Let,

\begin{equation}
    J(s, p, r) = s \cdot (p - k) - c(s, r) \cdot r + \beta \cdot V(s + r, p)
\end{equation}

such that, Equation (\ref{bellman_prod}) can be written as,

\begin{equation}
    V(s, p) = \max_{r_t \in [\underline{r}, \bar{r}]} J(s, p, r).
\end{equation}

Maximization of $J$ requires (first order condition),

\begin{equation} \label{J_foc_r}
    \frac{\partial J}{\partial r}(s, p, r)  = -\frac{\partial c}{\partial r}(s, r) \cdot r - c(s, r) + \beta \cdot \frac{\partial V}{\partial s}(s + r, p) = 0
\end{equation}

Assuming maximization in $r$, by the envelope theorem,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= (p - k) - \frac{\partial c}{\partial s}(s, r) \cdot r + \beta \cdot \frac{\partial V}{\partial s}(s + r, p)
    \end{split}
\end{equation}

Using Equation (\ref{J_foc_r}) on the first condition yields,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= (p - k) - \frac{\partial c}{\partial s}(s, r) \cdot r + \frac{\partial c}{\partial r}(s, r) \cdot r + c(s, r) \\
        &= (p - k) + \left[\frac{\partial c}{\partial r}(s, r) - \frac{\partial c}{\partial s}(s, r)\right] \cdot r + c(s, r)
    \end{split}
\end{equation}

Iterating forward yields,

\begin{equation}
    \frac{\partial V}{\partial s}(s + r, p)  = p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \cdot r + c(s + r, r)
\end{equation}

Using (\ref{J_foc_r}),

\begin{equation}
    \frac{\frac{\partial c}{\partial r}(s, r) \cdot r + c(s, r)}{\beta} =  p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \cdot r + c(s + r, r)
\end{equation}

\subsection{Stability of different cost functions}

The first order condition for $r$ requires that,

\begin{equation}
    \frac{\frac{\partial c}{\partial r}(s, r) \cdot r + c(s, r)}{\beta} =  p - k + \left[\frac{\partial c}{\partial r}(s + r, r) - \frac{\partial c}{\partial s}(s + r, r)\right] \cdot r + c(s + r, r)
\end{equation}

Any cost function with the trivial property that not ramping up is free, $c(s, 0) = 0$, is stable, $r = 0$, if and only if,


\begin{equation}
    p = k
\end{equation}

\subsection{Limiting behaviour as $c_1 \xrightarrow{} \infty$}\label{a:limiting}
\newcommand{\limc}{\lim_{c_1 \xrightarrow{} \infty}}

Assume a softplus cost function and notice that, letting

\begin{equation*}
    \sigm_{c_1}(x) = \frac{1}{1 + \exp(-c_1 \cdot x)},
\end{equation*}

we can write,

\begin{equation}
    \frac{\partial c}{\partial s}(s, r) = \sigm_{c_1}(s \cdot r) \cdot r \ \text{ and } \ \frac{\partial c}{\partial r}(s, r) = \sigm_{c_1}(s \cdot r) \cdot s
\end{equation}


Hence the two function that regulate the first order condition of the prosumer can be rewritten as,

\begin{equation}
    \begin{split}
        mc(r) &= \frac{\partial c}{\partial r}(s_t, r) \cdot r + c(s_t, r) \\
        &= \sigm_{c_1}(s_t \cdot r) \cdot s_t \cdot r_t + c(s_t, r)
    \end{split}
\end{equation}

and

\begin{equation}
    \begin{split}
        mb(r) &= p_t - k + \left[\frac{\partial c}{\partial r}(s_t + r, r) - \frac{\partial c}{\partial s_t}(s_t + r, r)\right] \cdot r + c(s_t + r, r)\\
        &= p_t - k + \left[\sigm_{c_1}(.) \cdot (s_t + r) - \sigm_{c_1}(.) \cdot r \right] \cdot r + c(s_t + r, r) \\
        &=p_t - k + \sigm_{c_1}((s_t + r) \cdot r) \cdot s_t \cdot r + c(s_t + r, r)
    \end{split}
\end{equation}

We can then study the behaviour of the two functions as $c_1 \xrightarrow{} \infty$. Given our model we can assume $s_t \geq 0$. First, notice that,

\begin{equation}
    \limc \sigm_{c_1}(x) = \begin{cases}
        1 & \text{if } x \geq 0 \\
        0 & \text{if } x < 0
    \end{cases}
\end{equation}

We can use this and the fact that $\limc c(s, r) = \max\{0, s \cdot r\}$, to evaluate,

\begin{equation}
    \limc mc(r) = \max \{0, 2s_t \cdot r\}
\end{equation}

and,

\begin{equation*}
    \limc mb(r) = p_t - k + \max\{0, (s_t + r) \cdot r\} + \begin{cases}
        s_t \cdot r &\text{if } r \in (-\infty, -s_t] \cup [0, \infty) \\
        0 &\text{if } r \in (-s_t, 0)
    \end{cases}
\end{equation*}

Using the optimization constraint, $r \in [-s_t, \infty)$, we obtain,

\begin{equation}
    \limc mb(r) = p_t - k + \begin{cases}
        2 s_t \cdot r + r^2 &\text{if } r > 0 \\
        0 &\text{if } r \in [-s_t, 0]
    \end{cases}
\end{equation}

Then, in the limit, we can rewrite the first order condition as,

\begin{equation*}
    \begin{split}
        \limc mc(r) &= \beta \limc mb(r) \\
        \frac{1}{\beta} \begin{cases}
            2 s_t \cdot r  &\text{if } r > 0 \\
            0 &\text{if } r \in [-s_t, 0]
        \end{cases} &= p_t - k  + \begin{cases}
            2 s_t \cdot r + r^2 &\text{if } r > 0 \\
            0 &\text{if } r \in [-s_t, 0]
        \end{cases}
    \end{split}
\end{equation*}

This allows us to find an approximate policy function. Namely, assuming producing is profitable, $p_t - k > 0$ and $s_t  > \frac{\beta}{1-\beta} \sqrt{p_t - k}$, we have the interior solution,

\begin{equation}
    r(s_t, p_t) = \frac{1}{2} \cdot \left[  \left(\frac{1 - \beta}{\beta} \cdot 2 s_t \right) - \sqrt{\left(\frac{1 - \beta}{\beta} \cdot 2 s_t \right)^2 - 4 \cdot \left(p_t - k\right)} \right].
\end{equation}

If, on the other hand, $s_t \leq \frac{\beta}{1-\beta} \sqrt{p_t - k}$, then $\beta \cdot mb(r) > mc(r)$, hence,

\begin{equation}
    r(s_t, p_t) = \frac{\beta}{1-\beta} \sqrt{p_t - k} - s_t
\end{equation}

Finally, if $p_t - k < 0$

\begin{equation}
    r(s_t, p_t) = -\gamma \cdot s
\end{equation}

\subsection{Evolution of excess demand} \label{a:ev_demand}

Using the definition of $X_t$, equation (\ref{x_inst}), we can rewrite,

\begin{equation}
    \begin{split}
        X_{t+1} &= M \cdot e_{t+1} - S_{t+1} \\
        S_{t+1} &= M \cdot e_{t+1} - X_{t+1} = R_t + S_t
    \end{split}
\end{equation}

hence,

\begin{equation}
    \begin{split}
        X_{t+1} - M \cdot e_{t+1} &= X_t - M \cdot e_t - R_t \\
        X_{t+1} &= X_t + M \cdot \left(e_{t+1} - e_t \right) -  R_t
    \end{split}
\end{equation}

\subsection{Provider optimization} \label{a:provider_optimization}

\subsubsection{Euler equation}

First assume that $X_t > 0$. The case for $X_t < 0$ is perfectly symmetric. Let the approximated ramp-up function be,

\begin{equation}
    \tilde{R}(p) \coloneqq a + b \cdot p = \E[R(p)]
\end{equation}

Each period the provider faces the problem,

\begin{equation*}
    \begin{split}
        \mathcal{L}(p, \Y, X) &= p \cdot X - \sum_{j \in N_{\mathcal{A}}(i)} Y^j_t \cdot P^j(\Y) \\
        &+ \lambda_t \cdot \left( X -  \sum_{j} Y_t^j \right) \\
        &+ \beta \cdot  V\left(X - \tilde{R} (p) \right)
    \end{split}
\end{equation*}

such that $V(X) = \max_{p, \Y} \mathcal{L}(p, \Y, X)$. The first order condition of $\mathcal{L}$ requires that,

\begin{equation} \label{foc_p}
    \begin{split}
        \frac{\partial}{\partial p} \mathcal{L} &= X - \beta \cdot V^\prime \left(X - \tilde{R}(p) \right) \cdot \tilde{R}^\prime(p) = 0 \\
        X &= \beta \cdot V^\prime \left(X - \tilde{R}(p) \right) \cdot \frac{\partial \tilde{R}}{\partial p}(p)
    \end{split}
\end{equation}

This first order condition implies that the benefit of charging a higher price today, which is today's demand $X$, should be equal to the cost of reducing the demand tomorrow, $\beta \cdot V^\prime \left(X - \tilde{R}(p) \right) \cdot \frac{\partial \tilde{R}}{\partial p}(p)$. The second first order condition requires that, for each neighbor $m$, 

\begin{equation} \label{foc_Y}
    \begin{split}
        \frac{\partial}{\partial Y^m} \mathcal{L} &= \sum_{j \in N_{\mathcal{A}}(i)} \frac{\partial (Y^j \cdot P^j(\Y))}{\partial Y^m} + \lambda = 0
    \end{split}
\end{equation}

We can distribute the partial derivative by noting that for each entry in the sum, $j$ not equal to $m$, we take the partial derivative of $P^j$ with respect to $Y^m$, and for the entry $m$ in the sum we need to use the chain rule. 

\begin{equation} \label{lambda}
    \begin{split}
        -\lambda &= \sum_{j \in N_{\mathcal{A}}(i)} \frac{\partial (Y^j \cdot P^j(\Y))}{\partial Y^m} \\
        &= Y^1 \cdot \frac{\partial P^1(\Y)}{\partial Y^m} + \ldots + \underbrace{\frac{\partial Y^m}{\partial Y^m} P^m(\Y) + Y^m \cdot \frac{\partial P^m(\Y)}{\partial Y^m} }_{\text{by the chain rule}} + Y^{m+1} \cdot \frac{\partial P^{m+1}(\Y)}{\partial Y^m} \ldots \\
        &= P^m(\Y) + \sum_{j \in N_{\mathcal{A}}(i)} Y^j \cdot \frac{\partial P^j(\Y)}{\partial Y^m}
    \end{split}
\end{equation} 

\subsubsection{Envelope}

By the envelope theorem,

\begin{equation} \label{env}
    V^\prime(X_t) = p_t + \lambda_t + \beta \cdot V^\prime(X_{t+1})
\end{equation}


Then combining Equations (\ref{env}) and (\ref{foc_p}), we obtain

\begin{equation}
    V^\prime(X_t) = p_t + \lambda_t + \frac{X_t}{\frac{\partial \tilde{R}}{\partial p}(p_t)}
\end{equation}

Iterating forward,

\begin{equation}
    V^\prime(X_{t+1}) = p_{t+1} + \lambda_{t+1} + \frac{X_{t+1}}{\frac{\partial \tilde{R}}{\partial p}(p_{t+1})}
\end{equation}

Using (\ref{foc_p}),

\begin{equation}
    \frac{X_t}{\beta \cdot \frac{\partial \tilde{R}}{\partial p}(p_t)} = p_{t+1} + \lambda_{t+1} + \frac{X_{t} - \tilde{R}(p_t)}{\frac{\partial \tilde{R}}{\partial p}(p_{t+1})}
\end{equation}

Finally, using the linearity of $\tilde{R}$, $\frac{\partial \tilde{R}}{\partial p} = b$,

\begin{equation}
    \begin{split}
        \frac{X_t}{\beta \cdot b} &=  \lambda_{t+1} + p_{t+1} + \frac{X_t}{b} - \frac{a +  b \cdot p_{t}}{b} \\
        p_{t+1} &= \left( p_t + \frac{a}{b} \right) +  \left( \frac{1 - \beta}{\beta} \right) \cdot \frac{X_t}{b} - \lambda_{t+1}
    \end{split}
\end{equation} 

\subsubsection{Bargaining solution} \label{a:barsol}

The Nash bargaining solution is such that

\begin{equation*}
    P^{(i, j)} = \arg \max_{P^{(i, j)}} \left\{\Pi_i \cdot \Pi_j \right\}.
\end{equation*}

The first order condition is such that,

\begin{equation*}
    \frac{\partial}{\partial P^{(i, j)}} \left( \Pi_i \cdot \Pi_j \right) = \frac{\partial \Pi_i}{\partial  P^{(i, j)}} \cdot \Pi_j + \Pi_i \cdot \frac{\partial \Pi_j}{\partial  P^{(i, j)}} = 0
\end{equation*}

Then, by symmetry, $\frac{\partial \Pi_j}{\partial  P^{(i, j)}} = - \frac{\partial \Pi_i}{\partial  P^{(i, j)}}$,

\begin{equation}
    \Pi_j = \Pi_i 
\end{equation}

Note that an equal division of the pie is consistent with the network bargaining model of \citein{Corominas-Bosch2004} with infinitely patient traders.


\subsection{Jacobian of bargaining price} \label{a:jacobian_p}

I will use the fact that,

\begin{equation}
    \diag(\Y) P = \diag(P) \Y = \Y \circ P \implies d \ \diag(\Y) P = \diag(P) \ d \Y
\end{equation}

First let $\matr{D} \coloneqq \diag(\Y)$ and $\matr{A} \coloneqq (2\matr{I} + \matr{G})^{-1} \Delta$. Noting that $\frac{d \matr{A}}{d \Y} = 0$,

\begin{equation}
    \begin{split}
        P &= \matr{D}^{-1} \matr{A} \\
        dP &= d\matr{D}^{-1} \matr{A} \\
        &= -\matr{D}^{-1} d \matr{D} \underbrace{\matr{D}^{-1} \matr{A}}_{P} \\
        &= -\matr{D}^{-1} \diag(P) \ d \Y
    \end{split}
\end{equation}