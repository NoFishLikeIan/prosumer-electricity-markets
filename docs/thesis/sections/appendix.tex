\documentclass[../main.tex]{subfiles}

\section{Optimization solutions}

\subsection{Producer optimization} \label{a:producer_optimization}

Let,

\begin{equation}
    J_{\psi}(s, p, r) = s \cdot p - c(s) \cdot r + \beta \cdot V(s + r, \psi \cdot p)
\end{equation}

such that, for a given $\psi$, Equation (\ref{bellman_prod}) can be written as,

\begin{equation}
    V(s, p) = \max_{r_t \in [\underline{r}, \bar{r}]} J_{\psi}(s, p, r).
\end{equation}

Maximization of $J_\psi$ requires (first order condition),

\begin{equation} \label{J_foc_r}
    \frac{\partial J_{\psi}}{\partial r}(s, p, r)  = -c(s) + \beta \cdot   \frac{\partial V}{\partial s}(s+r, \psi \cdot p)  = 0
\end{equation}

Assuming maximization in $r$, by the envelope theorem,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= p - c^\prime(s) \cdot r + \beta \cdot  \frac{\partial V}{\partial s}(s+r, \psi \cdot p)
    \end{split}
\end{equation}

Using Equation (\ref{J_foc_r}) on the first condition yields,

\begin{equation}
    \begin{split}
        \frac{\partial V}{\partial s}(s, p) &= p - c^\prime(s) \cdot r + c(s) \\
        \frac{\partial V}{\partial s}(s + r, \psi \cdot p) &= \psi \cdot p - c^\prime(s + r) \cdot r +  c(s + r)
    \end{split}
\end{equation}

Using (\ref{J_foc_r}),

\begin{equation}
    c^\prime(s + r) \cdot r - c(s+r) = \psi \cdot p - \frac{c(s)}{\beta}
\end{equation}

\subsection{Evolution of excess demand} \label{a:ev_demand}

Using the definition of $X_t$, equation (\ref{x_inst}), we can rewrite,

\begin{equation}
    \begin{split}
        X_{t+1} &= M \cdot e_{t+1} - S_{t+1} \\
        S_{t+1} &= M \cdot e_{t+1} - X_{t+1} = R_t + S_t
    \end{split}
\end{equation}

hence,

\begin{equation}
    \begin{split}
        X_{t+1} - M \cdot e_{t+1} &= X_t - M \cdot e_t - R_t \\
        X_{t+1} &= X_t + M \cdot \left(e_{t+1} - e_t \right) -  R_t
    \end{split}
\end{equation}

\subsection{Provider optimization} \label{a:provider_optimization}

\subsubsection{Euler equation}

Let the approximated ramp-up function be,

\begin{equation}
    \tilde{R}(p) \coloneqq a \cdot p + b
\end{equation}

and the Bellman maximization,

\begin{equation*}
    \begin{split}
        L(p_t, \Y_t) &\coloneqq p_t \cdot X_t - \sum_{j \in N_{\mathcal{A}}(i)} Y^j_t \cdot P^j(\Y_t) + \lambda_t \cdot \left( X_t -  \sum_{j} Y_t^j \right) \\
        &+ \beta \cdot  V\left(X_t - N \cdot \tilde{R} (p_t) \right)
    \end{split}
\end{equation*}

such that $V(X_t) = \max_{p_t, \Y_t} L(p_t, \Y_t)$. The first order condition of $L$ requires that,

\begin{equation} \label{foc_p}
    \frac{\partial}{\partial p_t} L = X_t - \beta \cdot V^\prime (X_{t+1}) \cdot N \cdot \tilde{R}^\prime(p_t)= 0
\end{equation}

Furthermore, the second first order condition requires that, for all $j$,

\begin{equation} \label{foc_Y}
    \frac{\partial}{\partial Y^j_t} L =  P^j(\Y_t) + \sum_{k \in N_{\mathcal{A}}(i)} Y^k_t \cdot \frac{\partial P^k}{\partial Y^j_t} (\Y_t) + \lambda_t = 0
\end{equation}

This implies that, $\forall j, l \in N_{\mathcal{A}}(i)$,

\begin{equation} \label{lambda_before}
    -\lambda_t = P^j(\Y_t) + \sum_{k \in N_{\mathcal{A}}(i)} Y^k_t \cdot \frac{\partial P^k}{\partial Y^j_t} (\Y_t) = P^l(\Y_t) + \sum_{k \in N_{\mathcal{A}}(i)} Y^k_t \cdot \frac{\partial P^k}{\partial Y^l_t} (\Y_t)
\end{equation}

\subsubsection{Envelope}

Assuming we are in the optimum,

\begin{equation} \label{env}
    V^\prime(X_t) = p_t + \lambda_t + \beta \cdot V^\prime(X_{t+1})
\end{equation}


Then combining Equations (\ref{env}) and (\ref{foc_p}), we obtain

\begin{equation}
    V^\prime(X_t) = p_t + \lambda_t + \frac{X_t}{N\cdot \tilde{R}^\prime(p_t)}
\end{equation}

Iterating forward,

\begin{equation}
    \begin{split}
        V^\prime(X_{t+1}) &= p_{t+1} + \lambda_{t+1} + \frac{X_{t+1}}{N\cdot \tilde{R}^\prime(p_{t+1})}
    \end{split}
\end{equation}

Using (\ref{foc_p}),

\begin{equation}
    \begin{split}
        \frac{X_t}{\beta \cdot N \cdot \tilde{R}^\prime(p_t)} &= p_{t+1} + \lambda_{t+1} + \frac{X_{t+1}}{N\cdot \tilde{R}^\prime(p_{t+1})} \\
        \frac{X_t}{\beta \cdot N \cdot \tilde{R}^\prime(p_t)} - \lambda_{t+1} &= p_{t+1} + \frac{X_t - N \cdot \tilde{R} (p_t)}{N\cdot \tilde{R}^\prime(p_{t+1})}
    \end{split}
\end{equation}

Using $\tilde{R}^\prime(p) = a$,

\begin{equation}
    \begin{split}
        \frac{X_t}{\beta \cdot N \cdot a} - \lambda_{t+1} &= p_{t+1} + \frac{X_t}{N\cdot a} - \frac{b +  a \cdot p_{t}}{a} \\
        p_{t+1} &= \lambda_{t+1} + \left( \frac{1 - \beta}{\beta} \right) \cdot \frac{X_t}{N \cdot a} - \frac{b +  a \cdot p_{t}}{a}
    \end{split}
\end{equation}

\subsection{Jacobian of bargaining price} \label{a:jacobian_p}

I will use the fact that,

\begin{equation}
    \diag(\Y) P = \diag(P) \Y = \Y \circ P \implies d \ \diag(\Y) P = \diag(P) \ d \Y
\end{equation}

First let $\matr{D} \coloneqq \diag(\Y)$ and $\matr{A} \coloneqq (2\matr{I} + \matr{G})^{-1} \Delta$. Noting that $\frac{d \matr{A}}{d \Y} = 0$,

\begin{equation}
    \begin{split}
        P &= \matr{D}^{-1} \matr{A} \\
        dP &= d\matr{D}^{-1} \matr{A} \\
        &= -\matr{D}^{-1} d \matr{D} \underbrace{\matr{D}^{-1} \matr{A}}_{P} \\
        &= -\matr{D}^{-1} \diag(P) \ d \Y \\
        \mathbb{J}\{P\} &= -\diag(\Y)^{-1} \diag(P) = - (P \oslash \Y) \iota
    \end{split}
\end{equation}

\subsection{Shadow price in vector form} \label{a:shadow_vector}

\begin{equation}
    \begin{split}
        &P + \left( \matr{G} \circ \mathbb{J}\{P\} \right)  \Y \\
        &P - \left( \matr{G} \circ \diag(\Y)^{-1} \diag(P) \right) \Y = 0
    \end{split}
\end{equation}